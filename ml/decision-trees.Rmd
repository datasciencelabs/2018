
```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
theme_set(theme_bw(base_size = 16))
library(readr)
```

# Decision Trees

We have described two types of machine learning algorithms. Linear approaches (linear regression, generalized linear models (GLM), discriminant analysis) and a smoothing approach (k-nearest neighbors). The linear approaches were limiting in that the partition of the prediction space had to be linear (in the case of QDA, quadratic). A limitation of the smoothing approach is that with a large number of predictors, we run into the problem of _the curse of dimensionality_.

## The Curse of Dimensionality

A useful way of understand the curse of dimensionality is by considering how large we have to make a neighborhood/window to include a given percentage of the data. For example, suppose we have one continuous predictor with equally spaced points in the [0,1] interval and we want to create windows that include 1/10-th of the data. Then it's easy to see that our windows have to be of size 0.1:

```{r, fig.width=10, fig.height=1}
rafalib::mypar()
x <- seq(0, 1, len=100)
y <- rep(1, 100)
plot(x, y, xlab = "", ylab = "", cex = 0.25, yaxt = "n", xaxt = "n", type = "n")
lines(x[c(15,35)], y[c(15,35)], col = "blue", lwd = 3)
points(x,y, cex = 0.25)
points(x[25],y[25], col = "blue", cex = 0.5, pch = 4)
text(x[c(15,35)], y[c(15,35)], c("[","]"))
```

Now, for two predictors, if we decide to keep the neighborhood just as small, 10% for each dimension, we include only 1 point:

```{r, echo=FALSE, fig.width=10, fig.height=10}
tmp <- expand.grid(1:10, 1:10)
x <- tmp[,1]
y <- tmp[,2]
plot(x, y, xlab = "",ylab = "", cex = 0.25, yaxt = "n", xaxt = "n",type = "n")
polygon(c(x[25]-0.5, x[25]-0.5, x[25]+0.5, x[25]+0.5),
        c(y[25]-0.5, y[25]+0.5, y[25]+0.5, y[25]-0.5), col = "blue")
points(x,y, cex = 0.25)
points(x[25], y[25], cex = 0.5, pch=4)
```

or, if we want to include 10% of the data we need to increase the window size to $\sqrt{10}$:

```{r, echo=FALSE, fig.width=10, fig.height=10}
plot(x, y, xlab = "",ylab = "", cex = 0.25, yaxt = "n", xaxt = "n", type = "n")
polygon(c(x[25]-sqrt(10)/2, x[25]-sqrt(10)/2, x[25]+sqrt(10)/2, x[25]+sqrt(10)/2),
        c(y[25]-sqrt(10)/2, y[25]+sqrt(10)/2, y[25]+sqrt(10)/2, y[25]-sqrt(10)/2),
        col = "blue")
points(x, y, cex = 0.25)
points(x[25], y[25], cex = 0.5, pch = 4)
```

To include 10% of the data in a case with $p$ features we need an interval for each predictor that covers $0.10^{1/p}$ of the total. This proportion gets close to 1 (including all the data and no longer smoothing) quickly:

```{r, echo=FALSE}
p <- 1:100
plot(p, .1^(1/p), ylim=c(0,1))
abline(h = 1)
```

Here we look at a set of elegant and versatile methods that adapt to higher dimensions and also allow these regions to take more complex shapes, but still produce models that are interpretable. These are very popular, well-known and studied methods. We will concentrate on Regression and Decision Trees and their extension to Random Forests.

### Regression Trees

Consider the olives dataset below. We show two measured predictors, `linoleic` (percent linoleic acid of sample) and `eicosenoic` (percent eicosenoic acid of sample). Suppose we wanted to predict the olive's region using these two predictors.

```{r, message=FALSE, warning=FALSE}
olives <- read.csv("https://raw.githubusercontent.com/datasciencelabs/data/master/olive.csv", as.is = TRUE) %>% tbl_df
names(olives)[1] <- "province"
region_names <- c("Southern Italy", "Sardinia", "Northern Italy")
olives <- olives %>% mutate(Region = factor(region_names[Region]))

p <- olives %>% ggplot(aes(eicosenoic, linoleic, fill = Region)) +
     geom_point(pch = 21)
p
```

Note that we can describe a classification algorithm that would work pretty much perfectly:

```{r}
p <- p + geom_vline(xintercept = 6.5) + 
         geom_segment(x = -2, y = 1053.5, xend = 6.5, yend = 1053.5)
p
```

The prediction algorithm inferred from the figure above is what we call a decision tree. If eicosnoic is larger than 6.5, predict Southern Italy. If not, then if linoleic is larger than 1054, predict Sardinia and Northern Italy otherwise. We can draw this decision tree like this:

```{r, echo=FALSE}
library(rpart)
olives <- olives %>% dplyr::select(., Region, linoleic, eicosenoic)
fit <- rpart(as.factor(Region)~., data = olives)
plot(fit, margin = 0.1)
text(fit, cex = 1)
```

Decision trees like this are often used in practice. For example, to decide if a person is at risk of having a heart attack, doctors use the following:

![](http://nargund.com/gsu/mgs8040/resource/dss/Simple%20Heuristics%20That%20Make%20Us%20Smart_files/todd.fig1.gif)


The general idea of the methods we are describing is to define an algorithm that uses data to create these tress. Regression and decision trees operate by predicting an outcome variable $Y$ by **partitioning** feature (predictor) space.

## Regression Trees

Let's start with the case of a continuous outcome. The general idea here is to build a decision tree and at the end of each _node_ we will have a different prediction $\hat{Y}$ for the outcome $Y$.

The **regression** tree model then:

1. Partitions space into $J$ non-overlapping regions, $R_1, R_2, \ldots, R_J$.
2. For every observation that falls within region $R_j$, predict the response as the mean of responses for training observations in $R_j$.

The important observation is that **Regression Trees create partitions recursively**.

For example, consider finding a good predictor $j$ to partition space along its axis. A recursive algorithm would look like this:

Find predictor $j$ and value $s$ that minimize RSS:

$$
\sum_{i:\, x_i \in R_1(j,s))} (y_i - \hat{y}_{R_1})^2 +
\sum_{i:\, x_i \in R_2(j,s))} (y_i - \hat{y}_{R_2})^2
$$

Where $R_1$ and $R_2$ are regions resulting from splitting observations on predictor $j$ and value $s$:

$$
R_1(j,s) = \{X|X_j < s\} \text{ and } R_2(j,s) \{X|X_j \geq s\}
$$

This is then applied recursively to regions $R_1$ and $R_2$. Within each region a prediction is made using $\hat{y}_{R_j}$ which is the mean of the response $Y$ of observations in $R_j$.

Let's take a look at what this algorithm does on the `Boston Housing` data set. This dataset contains information collected by the U.S Census Service concerning housing in the area of Boston, MA. It was obtained from the [StatLib archive](http://lib.stat.cmu.edu/datasets/boston). It contains information including `medv` (median value of owner-occupied homes in $1000's), `lstat` (% of individuals with lower socioeconomic status), `rm` (average number of rooms per dwelling), and `dis` (weighted distances to five Boston employment centres), among others with a total of 14 variables. 

The dataset is small in size with only 506 cases, but we'll use it for educational purposes.

```{r, message=FALSE, warning=FALSE}
library(tree)
library(MASS)
set.seed(1)
# Randomly sample half of the data for training
train = sample(1:nrow(Boston), nrow(Boston)/2) 
# Fit a regression tree using all of the available predictiors
fit = tree(medv ~ ., Boston, subset = train)  
# Print a summary of the tree
summary(fit)
# Use tree for prediction
preds <- predict(fit, newdata = Boston[-train,])
test = Boston[-train, "medv"]
```

Note that the output indicates only 3 of the predictors were used to construct the tree. Let's plot it and take a look:

```{r}
plot(fit, type = "uniform")
text(fit, cex = 1)
```

The tree suggests that lower values of `lstat` correspond to more expensive houses and predicts a median house price of $46,380 for larger homes in the suburbs in which residents have high socioeconomic status (`rm` >= 7.437 and `lstat` < 9.715). 

The idea behind the regression tree is that outcome $Y$ is estimated (or predicted) to be it's mean _within each of the data partitions_. Think of it as the conditional mean of $Y$ where conditioning is given by this region partitioning. 

We can also use the predictions made to calculate MSE (mean square error) to compare models. 
```{r}
plot(preds, test)
abline(0,1)
mean((preds-test)^2)
```

The test set MSE associated with this tree is 25.05. The square root of the MSE is 5.005, indicating this model leads to test predictions that are within approximately $5,005 of the true median home value for the suburb.

### Specifics of the regression tree algorithm

The recursive partitioning algorithm described above leads to a set of natural questions:

1. _When do we stop partitioning?_ 

We stop when adding a partition does not reduce MSE, or, when partition has too few training observations. Even then, trees built with this stopping criterion tend to _overfit_ training data. To avoid this, a post-processing step called **pruning** is used to make the tree smaller.

2. Why would a smaller tree tend to generalize better?

The `cv.tree` function is used to determine a reasonable tree depth for the given dataset. For this dataset it seems that a depth of 8 works well since it reaches the minimum error or "deviance" with that number:

```{r}
cv_boston = cv.tree(fit)
plot(cv_boston$size, cv_boston$dev, type = 'b')
```

However, if we decide to prune a tree we can do so using the `prune.tree()` function:
```{r}
prune_boston = prune.tree(fit, best = 5)
plot(prune_boston, type = "uniform")
text(prune_boston)
```

Let's calculate the test set MSE for the pruned tree:

```{r}
preds_prune <- predict(prune_boston, newdata = Boston[-train,])
test = Boston[-train, "medv"]
mean((preds_prune-test)^2)
```

The MSE of the pruned tree is greater than the original, so we should not prune this decision tree.

### Classification (Decision) Trees

Classification, or decision trees, are used in classification problems where the outcome is categorical. The same partitioning principle is used, but now each region predicts the **majority** class for training observations within that region. The recursive partitioning algorithm we saw previously requires a score function to choose predictors (and values) to partition with. In classification we could use a naive approach of looking for partitions that minimize training error. However, better performing approaches use more sophisticated metrics. Here are two of the most popular (denoted for leaf $m$):
 
  - **Gini Index**: $\sum_{k=1}^K \hat{p}_{mk}(1-\hat{p}_{mk})$, or
  
  - **Entropy**: $-\sum_{k=1}^K \hat{p}_{mk}\log(\hat{p}_{mk})$
  
where $\hat{p}_{mk}$ is the proportion of training observations in partition $m$ labeled as class $k$. Both of these seek to partition observations into subsets that have the same labels.

Let us look at how a classification tree performs on the digits example we examined before:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
detach("package:MASS", unload=TRUE)
url <- "https://raw.githubusercontent.com/datasciencelabs/data/master/hand-written-digits-train.csv"
if(!exists("digits")) digits <- read_csv(url)
digits <- digits %>% filter(label %in% c(2,7))
digits <- mutate(digits, label =  as.character(label)) %>% 
          mutate(label = ifelse(label=="2",0,1 ))
row_column <- expand.grid(row=1:28, col=1:28)
ind1 <- which(row_column$col <= 14 & row_column$row <=14)
ind2 <- which(row_column$col > 14 & row_column$row > 14)
ind <- c(ind1, ind2)
X <- as.matrix(digits[,-1])
X <- X>200
X1 <- rowSums(X[,ind1])/rowSums(X)
X2 <- rowSums(X[,ind2])/rowSums(X)
digits <- mutate(digits, X_1 = X1, X_2 = X2) %>% dplyr::select(., label, X_1, X_2) %>%
  mutate(label = as.factor(label))
y <- digits$label
x <- cbind(X1, X2)
library(caret)
fit <- knn3(x, y, 51)
GS <- 150
X1s <- seq(min(X1),max(X1),len=GS)
X2s <- seq(min(X2),max(X2),len=GS)
true_f <- expand.grid(X_1=X1s, X_2=X2s)
yhat <- predict(fit, newdata = true_f, type="prob")[,2]
true_f <- mutate(true_f, yhat=yhat)
f <- loess(yhat~X_1*X_2, data=true_f, 
           degree=1, span=1/5)$fitted
true_f <- true_f %>% mutate(f=f) 
rm(X,X1,X2,fit,GS,X1s,X2s,yhat,f)

true_f_plot <- true_f %>%
  ggplot(aes(X_1, X_2, fill=f))  +
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) + geom_raster()  + guides(fill=FALSE) +  
  stat_contour(aes(x=X_1,y=X_2,z=f), data=true_f, breaks=c(0.5),color="black",lwd=1.5)
true_f_plot
```


```{r, fig.width=14, fig.height=6, echo = FALSE}
inTrain <- createDataPartition(y = digits$label, p = 0.5)
digits_train <- slice(digits, inTrain$Resample1)
digits_test  <- slice(digits, -inTrain$Resample1)

fit <- tree(label ~ X_1 + X_2, data = digits_train)
plot(fit, type = "uniform")
text(fit, cex = 2, digits = 2)
```

We can see the prediction here:

```{r, message=FALSE, warning=FALSE}
f_hat_cart <- predict(fit, newdata = true_f)[,2]

p <-true_f %>% mutate(f = f_hat_cart) %>%
               ggplot(aes(X_1, X_2, fill = f)) +
               scale_fill_gradientn(colors = c("#F8766D","white","#00BFC4")) + 
               geom_raster() +  
               stat_contour(aes(x = X_1, y = X_2, z = f), 
               data = mutate(true_f, f = f_hat_cart),
               breaks = c(0.5), color = "black", lwd = 1.5) +
               guides(fill = FALSE)

library(gridExtra)
grid.arrange(true_f_plot, p, nrow=1)
```

We can again prune the tree if we wish, but in this case the pruned tree does not differ much from the original.

```{r}
pruned_fit <- prune.tree(fit)
plot(pruned_fit)
```

Here is what a pruned tree looks like:

```{r}
pruned_fit  <- prune.tree(fit, k = 160)
f_hat_cart2 <- predict(pruned_fit, newdata = true_f)[,2]
p <-true_f %>% mutate(f = f_hat_cart2) %>%
               ggplot(aes(X_1, X_2, fill = f)) +
               scale_fill_gradientn(colors = c("#F8766D","white","#00BFC4")) + 
               geom_raster() +  
               stat_contour(aes(x = X_1, y = X_2, z = f), 
               data = mutate(true_f, f = f_hat_cart2),
               breaks = c(0.5),color = "black", lwd = 1.5)
p
```



Classification trees have certain advantages that make them very useful. They are highly interpretable, even more so than linear models, are easy to visualize (if small enough), and they (maybe) model human decision processes and don't require that dummy predictors for categorical variables are used.

On the other hand, the greedy approach via recursive partitioning is a bit harder to train than linear regression. It may not always be the best performing method since it is not very flexible and is highly unstable to changes in training data. Below we will learn about the bootstrap to help with this.

## Bootstrap

Suppose the income distribution of your population is as follows:

```{r, echo = FALSE}
n <- 10^6
income <- 10^(rnorm(n, 4.656786, 0.4394738))
```

```{r}
hist(log10(income))
```

The population median is 
```{r}
m <- median(income)
m
```

Suppose we don't have access to the entire population but want to estimate the median $m$. We take a sample of 250 and estimate the population median $m$ with the sample median $M$:

```{r}
set.seed(1)
N <- 250
X <- sample(income, N)
M <- median(X)
M
```

Can we construct a confidence interval? What is the distribution of $M$?

From a Monte Carlo simulation we see that the distribution of $M$ is approximately normal with the following expected value and standard error:

```{r}
B <- 10^5
Ms <- replicate(B, {
  X <- sample(income, N)
  M <- median(X)
})
par(mfrow=c(1,2))
hist(Ms)
qqnorm(Ms)
qqline(Ms)
mean(Ms)
sd(Ms)
```

The problem here is that, as we have described before, in practice we do not have access to the distribution. In the past we have used the central limit theorem. But the CLT we studied applies to _averages_ and here we are interested in the _median_. 

The Bootstrap permits us to approximate a Monte Carlo simulation without access to the entire distribution. The general idea is relatively simple. We act as if the sample is the distribution and sample (with replacement) datasets of the same size. Then we compute the summary statistic, in this case the median, on this _bootstrap sample_. 

There is theory telling us that the distribution of the statistics obtained with bootstrap samples approximate the distribution of our actual statistic. This is how we construct bootstrap samples and an approximate distribution:


```{r}
B <- 10^5
M_stars <- replicate(B, {
  X_star <- sample(X, N, replace = TRUE)
  M_star <- median(X_star)
})
```

Now we can check how close it is to the actual distribution
```{r}
qqplot(Ms, M_stars)
abline(0,1)  
```

We see it is not perfect but it provides a decent approximation:

```{r}
quantile(Ms, c(0.05, 0.95))
quantile(M_stars, c(0.05, 0.95))
```

This is much better than what we get if we mindlessly use the CLT:
```{r}
median(X) + 1.96 * sd(X)/sqrt(N) * c(-1,1)
```


If we know the distribution is normal, we can use the bootstrap to estimate the mean:
```{r}
mean(Ms) + 1.96*sd(Ms)*c(-1,1)
mean(M_stars) + 1.96*sd(M_stars)*c(-1,1)
```



## Random Forests

Random Forests are a **very popular** approach that address the shortcomings of decision trees via re-sampling of the training data. Their goal is to improve prediction performance and reduce instability by _averaging_ multiple decision trees (a forest constructed with randomness). It has two features that help accomplish this.

The first trick is *Bagging* (bootstrap aggregation)
General scheme:

  1. Build many decision trees $T_1, T_2, \ldots, T_B$ from training set
  2. Given a new observation, let each $T_j$ predict $\hat{y}_j$
  3. For regression: predict average $\frac{1}{B} \sum_{j=1}^B \hat{y}_j$,
     for classification: predict with majority vote (most frequent class)
     
But how do we get many decision trees from a single training set?

For this we use the _bootstrap_. To create $T_j, \, j=1,\ldots,B$ from a training set of size $N$:

a) Create a bootstrap training set by sampling $N$ observations from training set **with replacement**
b) Build a decision tree from the bootstrap training set

Let's look at this using the Boston housing dataset. We fit a Random Forest by using the `randomForest()` function. Here, `mtry = 13` indicates all 13 predictors should be considered for each split of the tree - in other words, bagging should be done. 

```{r, message=FALSE, warning=FALSE}
library(randomForest)
library(MASS)
fit_bag <- randomForest(medv ~ ., data = Boston, mtry = 13)
fit_bag
```

How well does it perform on the test set?
```{r}
preds_bag = predict(fit_bag, newdata = Boston[-train,])
plot(preds_bag, test)
abline(0,1)
mean((preds_bag - test)^2)
```

Much, much better than the one decision tree we used above. 

We can grow a random forest in the same way, except now we use a smaller value for `mtry`. The default for regression trees is $p/3$ ($p$ is the number of predictors) and the default for classification trees is $\sqrt p$. Let's try `mtry = 6`.

```{r}
fit_rf <- randomForest(medv ~ ., data = Boston, mtry = 6, importance = TRUE)
preds_rf = predict(fit_rf, newdata = Boston[-train,])
mean((preds_rf - test)^2)
```

Looks like the MSE for this random forest is worse with fewer predictors and the one using all of the predictors performs the best.

The second Random Forests feature is to use a random selection of features to split when deciding partitions. Specifically, when building each tree $T_j$, at each recursive partition only consider a _randomly selected subset of predictors_ to check for the best split. This reduces correlation between trees in a forest, improving prediction accuracy.

Here is the random forest fit for the digits data:

```{r, warning=FALSE, message=FALSE}
detach("package:MASS", unload=TRUE)
library(randomForest)
fit <- randomForest(label ~ X_1 + X_2, data = digits_train)
f_hat_rf <- predict(fit, newdata = true_f, type = "prob")[,2]

p <-true_f %>% mutate(f = f_hat_rf) %>%
               ggplot(aes(X_1, X_2, fill = f)) +
               scale_fill_gradientn(colors = c("#F8766D","white","#00BFC4")) + 
               geom_raster() + 
               stat_contour(aes(x = X_1, y = X_2, z = f), 
               data = mutate(true_f, f = f_hat_rf),
               breaks = c(0.5), color = "black", lwd = 1.5) +
               guides(fill = FALSE)

library(gridExtra)
grid.arrange(true_f_plot, p, nrow = 1)
```

We can control the "smoothness" of the random forest estimate in several ways. One way is to limit the size of each node. We can require the number of points per node to be larger:

```{r}

fit <- randomForest(as.factor(label) ~ X_1  +X_2,
                    nodesize = 250,
                    data = digits_train)
f_hat_rf <- predict(fit, newdata = true_f, type = "prob")[,2]

p <-true_f %>% mutate(f = f_hat_rf) %>%
               ggplot(aes(X_1, X_2, fill = f))  +
               scale_fill_gradientn(colors = c("#F8766D","white","#00BFC4")) + 
               geom_raster() +  
               stat_contour(aes(x = X_1, y = X_2, z = f), 
               data = mutate(true_f, f = f_hat_rf),
               breaks = c(0.5), color = "black", lwd = 1.5) +
               guides(fill = FALSE)

p
```


We can compare the results:

```{r}
library(caret)
get_accuracy <- function(fit){
  pred <- predict(fit, newdata = digits_test, type = "class")
  confusionMatrix(table(pred = pred, true = digits_test$label))$overall[1]
}
fit <- tree(label ~ X_1 + X_2, data = digits_train)
get_accuracy(fit)

fit <- randomForest(label ~ X_1 + X_2, data = digits_train)
get_accuracy(fit)

fit <- randomForest(label ~ X_1 + X_2,
                    nodesize = 250,
                    data = digits_train)
get_accuracy(fit)
```


A disadvantage of random forests is that we lose interpretability. However, we can use the fact that a bootstrap sample was used to construct trees to measure _variable importance_ from the random forest.

Let's see this using all the digits data:

```{r, message=FALSE, warning=FALSE}
url <- "https://raw.githubusercontent.com/datasciencelabs/data/master/hand-written-digits-train.csv"
digits <- read_csv(url)

digits <- mutate(digits, label = as.factor(label))

inTrain   <- createDataPartition(y = digits$label, p = 0.9)
train_set <- slice(digits, inTrain$Resample1)
test_set  <- slice(digits, -inTrain$Resample1)

fit <- randomForest(label~., ntree = 100, data = train_set)
```

How well does it do?

```{r}
pred <- predict(fit, newdata = test_set, type = "class")
confusionMatrix(table(pred = pred, true = test_set$label))
```

Here is a table of _variable importance_ for the random forest we just constructed.


```{r, echo=TRUE, results="asis"}
library(knitr)
variable_importance <- importance(fit) 
tmp <- data_frame(feature = rownames(variable_importance),
                  Gini = variable_importance[,1]) %>%
                  arrange(desc(Gini))
kable(tmp[1:10,])
```

We can see where the "important" features are:

```{r}
expand.grid(Row = 1:28, Column = 1:28) %>%
            mutate(value = variable_importance[,1]) %>%
            ggplot(aes(Row, Column, fill = value)) +
            geom_raster() +
            scale_y_reverse() 
```


And a barplot of the same data showing only the most predictive features:

```{r}
tmp %>% filter(Gini > 200) %>%
        ggplot(aes(x=reorder(feature, Gini), y=Gini)) +
        geom_bar(stat='identity') +
        coord_flip() + xlab("Feature") +
        theme(axis.text=element_text(size=8))
```

### Tree-based methods summary

Tree-based methods are very interpretable _prediction_ models. For which some inferential tasks are possible (e.g., variable importance in random forests), but are much more limited than the linear models we saw previously. These methods are very commonly used across many application domains and Random Forests often perform at state-of-the-art for many tasks.
